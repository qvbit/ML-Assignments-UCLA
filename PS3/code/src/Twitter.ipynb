{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Twitter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 a-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nabeelnauman/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from twitter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split:\n",
      "Shape of X: (630, 1811)\n",
      "Shape of y: (630,)\n",
      "After split: \n",
      "Shape of Xtrain: (560, 1811)\n",
      "Shape of Xtest: (70, 1811)\n",
      "Shape of ytrain: (560,)\n",
      "Shape of ytest: (70,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "    \n",
    "# read the tweets and its labels   \n",
    "dictionary = extract_dictionary('../data/tweets.txt')\n",
    "X = extract_feature_vectors('../data/tweets.txt', dictionary)\n",
    "y = read_vector_file('../data/labels.txt')\n",
    "\n",
    "print(\"Before split:\")\n",
    "\n",
    "print(\"Shape of X: \" + str(X.shape))\n",
    "print(\"Shape of y: \" + str(y.shape))\n",
    "\n",
    "print(\"After split: \")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.11, random_state = 42)\n",
    "\n",
    "print(\"Shape of Xtrain: \" + str(X_train.shape))\n",
    "print(\"Shape of Xtest: \" + str(X_test.shape))\n",
    "print(\"Shape of ytrain: \" + str(y_train.shape))\n",
    "print(\"Shape of ytest: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after the split, the examples have been split into the same ratio as given by the problem. In particular, X_train is now a 560 x 1811 array and X_test is a 70 x 1811 array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_list = [\"accuracy\", \"f1_score\", \"auroc\"]\n",
    "kf = StratifiedKFold(y = y_train, n_folds = 5) #create indices of 5-vold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason it's a good idea to maintain the class proportions across the folds is because it resembles the original distrubution the best. Our original distrubution is our best \"guess\" of what the true underlying distrubution looks like and we assume it's proportional are similar to that of the training data. If the proportions aren't similar to the train data, then this is not representative the true underlying distrubution and may make performance on the test set worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 c-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Hyperparameter Selection based on accuracy:\n",
      "10.0\n",
      "The performance with this value of C was: 0.783601\n",
      "{0.001: 0.7081825662577874, 0.01: 0.7064126547533627, 0.1: 0.7207316830104441, 1.0: 0.7800133255885469, 10.0: 0.7836005569412648, 100.0: 0.7836005569412648}\n",
      "Linear SVM Hyperparameter Selection based on f1_score:\n",
      "10.0\n",
      "The performance with this value of C was: 0.834326\n",
      "{0.001: 0.8137795566454397, 0.01: 0.8122673615234884, 0.1: 0.8092921769811928, 1.0: 0.8318029165265688, 10.0: 0.8343262047121867, 100.0: 0.8343262047121867}\n",
      "Linear SVM Hyperparameter Selection based on auroc:\n",
      "10.0\n",
      "The performance with this value of C was: 0.734549\n",
      "{0.001: 0.5, 0.01: 0.5062629399585921, 0.1: 0.5962365997858499, 1.0: 0.7309176038160226, 10.0: 0.7345489002783662, 100.0: 0.7345489002783662}\n"
     ]
    }
   ],
   "source": [
    "best_c_dict = [{}, {}, {}]\n",
    "for i, metric in enumerate(metric_list):\n",
    "    best_c, best_c_dict[i] = select_param_linear(X=X, y=y, kf=kf, metric=metric)\n",
    "    print(best_c_dict[i])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best C was found to be 10 and the corresponding performance with the given metric is also reported. In addition, all C values with their corresponding performance are reported for all 3 metrics. C = 10 was the best for all of them. Interstingly, the performance seems to get capped off at some maximum value after C = 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel=\"linear\", C=10)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 b-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8142857142857143, 'f1_score': 0.8505747126436781, 'auroc': 0.7976190476190477}\n"
     ]
    }
   ],
   "source": [
    "perf_dict = {} \n",
    "for metric in metric_list:\n",
    "    perf_dict[metric] = performance_test(clf, X_test, y_test, metric)\n",
    "    \n",
    "print(perf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the given performance scores for the three given metrics using the optimal hyperparameter settings for the SVM model found in 4.2 (i.e. C = 10)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
